{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /home/saumya/.local/lib/python3.10/site-packages (from scikit-learn) (2.2.2)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/saumya/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/saumya/.local/lib/python3.10/site-packages (from matplotlib) (2.2.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting pillow>=8\n",
      "  Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/saumya/.local/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Collecting pandas>=1.2\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: tzdata, pillow, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.8 matplotlib-3.10.0 pandas-2.2.3 pillow-11.1.0 seaborn-0.13.2 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import load_wine \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0/input_dim)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0/hidden_dim)\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = np.dot(x, self.W1) + self.b1\n",
    "        hidden = self.relu(hidden)\n",
    "        output = np.dot(hidden, self.W2) + self.b2\n",
    "        return output\n",
    "    \n",
    "    def get_gradients(self, x, grad_output):\n",
    "        hidden_pre = np.dot(x, self.W1) + self.b1\n",
    "        hidden = self.relu(hidden_pre)\n",
    "        \n",
    "        grad_W2 = np.dot(hidden.T, grad_output)\n",
    "        grad_b2 = np.sum(grad_output, axis=0)\n",
    "        \n",
    "        grad_hidden = np.dot(grad_output, self.W2.T)\n",
    "        grad_hidden[hidden_pre <= 0] = 0 \n",
    "        \n",
    "        grad_W1 = np.dot(x.T, grad_hidden)\n",
    "        grad_b1 = np.sum(grad_hidden, axis=0)\n",
    "        \n",
    "        return {'W1': grad_W1, 'b1': grad_b1, 'W2': grad_W2, 'b2': grad_b2}\n",
    "    \n",
    "    def update_params(self, grads, learning_rate):\n",
    "        self.W1 -= learning_rate * grads['W1']\n",
    "        self.b1 -= learning_rate * grads['b1']\n",
    "        self.W2 -= learning_rate * grads['W2']\n",
    "        self.b2 -= learning_rate * grads['b2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gating:\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        self.W = np.random.randn(input_dim, num_experts) * np.sqrt(2.0/input_dim)\n",
    "        self.b = np.zeros(num_experts)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = np.dot(x, self.W) + self.b\n",
    "        return self.softmax(logits)\n",
    "    \n",
    "    def get_gradients(self, x, grad_output):\n",
    "        grad_W = np.dot(x.T, grad_output)\n",
    "        grad_b = np.sum(grad_output, axis=0)\n",
    "        return {'W': grad_W, 'b': grad_b}\n",
    "    \n",
    "    def update_params(self, grads, learning_rate):\n",
    "        self.W -= learning_rate * grads['W']\n",
    "        self.b -= learning_rate * grads['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfExperts:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts):\n",
    "        self.experts = [Expert(input_dim, hidden_dim, output_dim) \n",
    "                       for _ in range(num_experts)]\n",
    "        self.gating = Gating(input_dim, num_experts)\n",
    "        self.num_experts = num_experts\n",
    "    \n",
    "    def forward(self, x):\n",
    "        expert_outputs = np.stack([expert.forward(x) for expert in self.experts])\n",
    "        expert_outputs = np.transpose(expert_outputs, (1, 0, 2))\n",
    "        gating_weights = self.gating.forward(x)\n",
    "        final_output = np.sum(expert_outputs * gating_weights[..., np.newaxis], axis=1)\n",
    "        return final_output, gating_weights, expert_outputs\n",
    "    \n",
    "    def train_step(self, x, y, learning_rate):\n",
    "        output, gating_weights, expert_outputs = self.forward(x)\n",
    "        loss = np.mean((output - y) ** 2)\n",
    "        batch_size = x.shape[0]\n",
    "        grad_output = 2 * (output - y) / batch_size\n",
    "        expert_grads = []\n",
    "        for i in range(self.num_experts):\n",
    "            expert_grad = grad_output * gating_weights[:, i:i+1]\n",
    "            expert_grads.append(self.experts[i].get_gradients(x, expert_grad))\n",
    "        \n",
    "        grad_gating = np.sum(grad_output[..., np.newaxis] * expert_outputs, axis=2)\n",
    "        gating_grads = self.gating.get_gradients(x, grad_gating)\n",
    "        \n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert.update_params(expert_grads[i], learning_rate)\n",
    "        self.gating.update_params(gating_grads, learning_rate)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wine_quality():\n",
    "    wine = load_wine()\n",
    "    X = wine.data\n",
    "    y = wine.target.reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_moe(model, X_train, y_train, epochs=100, learning_rate=0.01, batch_size=32):\n",
    "    n_samples = X_train.shape[0]\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        epoch_losses = []\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_shuffled[i:i + batch_size]\n",
    "            y_batch = y_shuffled[i:i + batch_size]\n",
    "            \n",
    "            loss = model.train_step(X_batch, y_batch, learning_rate)\n",
    "            epoch_losses.append(loss)\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.4366\n",
      "Epoch 10, Loss: 0.2098\n",
      "Epoch 20, Loss: 0.1408\n",
      "Epoch 30, Loss: 0.0993\n",
      "Epoch 40, Loss: 0.0888\n",
      "Epoch 50, Loss: 0.0720\n",
      "Epoch 60, Loss: 0.0645\n",
      "Epoch 70, Loss: 0.0557\n",
      "Epoch 80, Loss: 0.0614\n",
      "Epoch 90, Loss: 0.0491\n",
      "Test MSE: 0.0706\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_train, X_test, y_train, y_test = load_wine_quality()\n",
    "    input_dim = X_train.shape[1]\n",
    "    hidden_dim = 32\n",
    "    output_dim = 1\n",
    "    num_experts = 3\n",
    "    model = MixtureOfExperts(input_dim, hidden_dim, output_dim, num_experts)\n",
    "    losses = train_moe(model, X_train, y_train, epochs=100, learning_rate=0.01)\n",
    "    test_output, _, _ = model.forward(X_test)\n",
    "    test_mse = np.mean((test_output - y_test) ** 2)\n",
    "    print(f\"Test MSE: {test_mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
